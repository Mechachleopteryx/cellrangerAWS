#! /bin/bash

set -o nounset -o pipefail -o errexit -x


# Inputs
INPUT="none"
CONFIG="none"
S3="none"
KEY="none"
CLEAN_UP=false
PROJECT="cellranger"
TRANSFER_DATA=true
CHILD_LOCK=true

EC2_TYPE="c5.12xlarge"
AMI="ami-06f3276cab06d4b5f"
MANUAL_MODE=false

ZONE="us-west-2a"
SECURITY_GROUP="sg-00cf9318d9719735d"
SUBNET="subnet-083a3a855e543ee1c"
EC2_USER="ubuntu"


# Usage message
usage() {
    echo """
USAGE
"$0" [OPTIONS]

OPTIONS
-h, display this message.

-i, path to local directory or S3 bucket containing fastq files.
-c, path to config.yaml ...OR S3 BUCKET.
-s, name of S3 bucket.

-k, path to AWS private key.
-t, EC2 instance type (default is "$EC2_TYPE").
-a, ID for machine image configured to run cellranger (default is "$AMI").
-f, terminate running instances/volumes and start new job.
-m, run in manual mode, this transfers data to an EC2 instance and exits.
    """
}





# Parse arguments =============================================================

while getopts ":hi:c:s:k:t:p:a:fm" args
do
    case "$args" in
        h)
            usage
            exit 0
            ;;
        i) 
            INPUT="$OPTARG"

            if [[ "$INPUT" =~ ^s3://.+ ]]
            then
                TRANSFER_DATA=false
                S3="$INPUT"
            fi
            ;;
        c) CONFIG="$OPTARG" ;;
        s) S3="$OPTARG" ;;
        k) KEY="$OPTARG" ;;
        t) EC2_TYPE="$OPTARG" ;;
        p) PROJECT="$OPTARG" ;;
        a) AMI="$OPTARG" ;;
        f) CLEAN_UP=true ;;
        m) MANUAL_MODE=true ;;
        :)
            echo -e "\nERROR: -$OPTARG requires an argument"

            usage
	    exit 1
            ;;
	*) 
            usage
	    exit 1
            ;;
    esac
done

if [[ "$INPUT" == "none" || "$CONFIG" == "none" || "$KEY" == "none" ]]
then
    echo -e "\nERROR: Arguments are required for -i, -c, and -k."

    usage
    exit 1
fi





# Check instance/volume status =================================================

check_status() {
    local input_fun="$1"
    local id="$2"
    local num=60

    if [[ "$#" == 3 ]]
    then
        local num="$3"
    fi

    echo -e "\nChecking for $input_fun $id..."

    for i in $(seq 1 "$num")
    do
        if "$input_fun" "$id" | grep --quiet "$id"
        then
            return 0
        fi

        sleep 10
    done

    echo -e "\nERROR: Timed out waiting for $input_fun $id."

    exit 1
}





# Check for the undead =========================================================

check_for_undead() {
    local kill_job=false
    

    # Terminate EC2 instances from previous jobs
    local running_ec2s=$(
        aws ec2 describe-instances \
            --filters Name=instance-state-name,Values=running
    )

    if [[ "$running_ec2s" =~ "InstanceId" ]]
    then
        local ec2_ids=$(
            echo $running_ec2s \
	        | grep "InstanceId" \
                | grep -E -o "i\-[[:alnum:]]+"
        )

	if "$CLEAN_UP"
        then
            clean_up "$ec2_ids"
	
	else
            echo -e "\nERROR: An EC2 instance from a previous job is still running, to "
	    echo "terminate this instance and start a new job, include the -f option."

	    local kill_job=true
        fi
    fi
    


    # Delete available EBS volumes from previous jobs
    available_vols=$(
        aws ec2 describe-volumes \
            --filters Name=status,Values=available \
    )

    if [[ "$available_vols" =~ "VolumeId" ]]
    then
        local vol_ids=$(
	    echo $available_vols \
                | grep "VolumeId" \
                | grep -E -o "vol\-[[:alnum:]]+"
	)

	if "$CLEAN_UP"
        then
            clean_up "$vol_ids"
        
        else
            echo -e "\nERROR: An EBS volume from a previous job is still available, "
	    echo "to delete this volume and start a new job, include the -f option."

	    local kill_job=true
        fi
    fi

    if "$kill_job"
    then
        exit 1
    fi
}





# Transfer data to S3 bucket ===================================================

transfer_to_s3() {
    local input="$1"
    local s3="$2"

    if ! aws s3 ls "$s3" > /dev/null
    then
        bucket_name=$(echo "$s3" | sed 's/s3:\/\///g')

        aws s3api create-bucket \
            --bucket "$bucket_name"
    fi

    for fq in "$input"*.fastq.gz
    do
        aws s3 cp "$fq" "$s3"
    done
}





# Launch EC2 instance ==========================================================
# When using default VPC the instance does not have internet access. To resolve
# this create new VPC, security group, subnet, and internet gateway. Activate
# public IP for subnet, activate public DNS for VPC, attach VPC to internet
# gateway, modify route table for VPC.

launch_ec2() {
    local ami="$1"
    local ec2_type="$2"
    local zone="$3"
    local security_group="$4"
    local subnet="$5"
    local key_name=$(basename -s .pem "$KEY")

    echo -e "\nLaunching EC2 $EC2_TYPE instance..."



    # Launch EC2 instance
    # Public DNS is not listed in run-instances output, this must be retrieved
    # separately
    ec2_id=$(
        aws ec2 run-instances \
            --instance-type "$ec2_type" \
            --image-id "$ami" \
            --placement AvailabilityZone="$zone" \
            --security-group-ids "$security_group" \
            --subnet-id "$subnet" \
            --key-name "$key_name" \
            --associate-public-ip-address \
            | grep -E -o "InstanceId\": \"i\-[[:alnum:]]+" \
            | grep -E -o "i\-[[:alnum:]]+"
    )

    running_ec2() {
        aws ec2 describe-instances \
            --instance-id "$1" \
            --filters Name=instance-state-name,Values=running
    }

    check_status running_ec2 "$ec2_id"

    echo -e "\nEC2 instance $ec2_id has been successfully launched."
}





# Retrieve public DNS for EC2 instance and check connectivity ==================

get_dns() {
    local ec2="$1"

    echo -e "\nAttempting to make contact with EC2 instance $ec2..."



    # Retrieve public IP to check connectivity
    local ip=$(
        aws ec2 describe-instances \
            --instance-id "$ec2" \
            | grep -E "PublicIpAddress" \
            | grep -E -o "[0-9\.]+"
    )



    # Ping IP
    for i in $(seq 1 20)
    do
        local packet_loss=$(
            ping -c 10 "$ip" \
                | grep -E -o "[0-9]+% packet loss" \
                | grep -E -o "[0-9]+"
        )

        # If IP responds to ping, return public DNS
        if [[ "$packet_loss" < 100 ]]
        then
            local dns=$(
                aws ec2 describe-instances \
                    --instance-id "$ec2" \
                    | grep -E -o "ec2\-[[:alnum:]\.\-]+compute.amazonaws.com" \
                    | head -n 1
            )

            EC2_SSH="$EC2_USER@$dns"

            echo -e "\nConnection has been established with EC2 instance $ec2."
	    echo "Use the following command to manually connect to the instance:"
	    echo "ssh -i $KEY $EC2_SSH"

            return 0
        fi
    done



    # If IP does not respond to ping, terminate instance
    aws ec2 terminate-instances \
        --instance-ids "$ec2"

    echo -e "ERROR: Unable to connect, terminating EC2 instance $ec2..."

    exit 1
}





# Create and mount EBS volume on EC2 instance ==================================

create_ebs() {
    local input="$1"
    local ec2="$2"
    local zone="$3"
    local s3="$4"

    echo -e "Creating EBS volume..."



    # Get total file size
    local data_size=$(
        aws s3 ls "$s3" --summarize --human-readable \
            | grep "Total Size" \
            | grep -E -o "[0-9].+$"
    )

    local vol_size=$(
        echo "$data_size" \
            | grep -E -o "^[0-9]+"
    )

    local vol_size=$(expr "$vol_size" \* 3)

    if ! echo "$data_size" | grep -E "GiB|TiB" --quiet
    then
        local vol_size=10
    fi

    if [[ "$vol_size" < 10 ]]
    then
        local vol_size=10
    fi



    # Get list of attached devices, need this to identify attached EBS volume
    local old_devs=$(
        ssh -o "StrictHostKeyChecking no" -i "$KEY" "$EC2_SSH" \
            sudo fdisk -l \
                | grep -E -o "/dev/[[:alnum:]]+" \
                2>&1
    )

    # Keep old_devs unquoted here
    local old_devs=$(
        echo $old_devs \
            | sed 's/ /|/g'
    )



    # Create EBS volume
    vol_id=$(
        aws ec2 create-volume \
            --size "$vol_size" \
            --availability-zone "$zone" \
            | grep -E -o "vol\-[[:alnum:]]+"
    )
        
    available_vol() {
        aws ec2 describe-volumes \
            --volume-ids "$1" \
            --filters Name=status,Values=available
    }

    check_status available_vol "$vol_id"



    # Attach EBS volume
    aws ec2 attach-volume \
        --instance-id "$ec2" \
        --volume-id "$vol_id" \
        --device /dev/sdh \
        > /dev/null

    attached_vol() {
        aws ec2 describe-volumes \
            --volume-ids "$1" \
            --filters Name=attachment.status,Values=attached
    }

    check_status attached_vol "$vol_id"



    # Identify device path for EBS volume
    local new_devs=$(
        ssh -i "$KEY" "$EC2_SSH" \
            sudo fdisk -l \
                | grep -E "^Disk .+$vol_size GiB" \
                | grep -E -o "/dev/[[:alnum:]]+" \
                2>&1
    )
    
    local dev_path=$(
        echo "$new_devs" \
            | grep -E -v "$old_devs"
    )



    # Format and mount EBS volume
    ssh -i "$KEY" "$EC2_SSH" bash <<EOF
        sudo parted "$dev_path" mklabel gpt > /dev/null 2>&1
        sudo parted -a opt "$dev_path" mkpart primary ext4 0% 100% > /dev/null 2>&1
        sudo mkfs.ext4 -F "$dev_path" > /dev/null 2>&1

        sudo mkdir -p /mnt/EBS
        sudo mount "$dev_path" /mnt/EBS
        sudo mkdir -p /mnt/EBS/DATA
        sudo chmod -R 777 /mnt/EBS
EOF

    echo -e "\nEBS volume $vol_id ($vol_size GB) was created and mounted to EC2 instance $ec2."
}





# Transfer data from S3 bucket to EBS volume ==================================

transfer_to_ec2() {
    local s3="$1"
    local vol="$2"
    local snap_name="$3"



    # Configure AWS on EC2 instance
    local iam_id=$(
        cat "$HOME/.aws/credentials" \
            | grep -A 2 "default" \
            | grep "aws_access_key_id" \
	    | grep -E -o "[[:alnum:]]+$"
    )

    local iam_key=$(
        cat "$HOME/.aws/credentials" \
            | grep -A 2 "default" \
            | grep "aws_secret_access_key" \
            | grep -E -o "[[:alnum:]/\+]+$"
    )

    local aws="/home/$EC2_USER/.local/bin/aws"

    ssh -i "$KEY" "$EC2_SSH" \
        "$aws" configure set aws_access_key_id "$iam_id"

    ssh -i "$KEY" "$EC2_SSH" \
        "$aws" configure set aws_secret_access_key "$iam_key"



    # Transfer files to EBS volume
    scp -i "$KEY" "$CONFIG" "$EC2_SSH:~/"

    local s3_fqs=$(
        aws s3 ls "$s3" \
            | grep -E -o "[[:alnum:]_\-\.]+.fastq.gz"
    )

    # Leave s3_fqs unquoted
    for fq in $s3_fqs
    do 
        ssh -i "$KEY" "$EC2_SSH" \
            "$aws" s3 cp "$s3/$fq" /mnt/EBS/DATA
    done
   
    wait

    sleep 60
}





# Terminate EC2 instance and delete EBS volume =================================

clean_up() {


    # Terminate EC2 instance
    if [[ "$1" =~ ^i\-[[:alnum:]]+$ ]]
    then
        local ec2="$1"

        echo -e "\nTerminating EC2 instance $ec2..."

        aws ec2 terminate-instances \
            --instance-ids "$ec2" \
            > /dev/null

        terminated_ec2() {
            aws ec2 describe-instances \
                --instance-id "$1" \
                --filters Name=instance-state-name,Values=terminated
        }

        check_status terminated_ec2 "$ec2" 
	
        echo -e "\nEC2 instance $ec2_id has been terminated."   
    fi



    # Delete EBS volume
    if [[ "$1" =~ ^vol\-[[:alnum:]]+$ ]] || [[ "$#" == 2 ]]
    then
        local vol="$1"

        # Delete EBS volume
        echo -e "\nDeleting EBS volume $vol..."

        aws ec2 delete-volume \
            --volume-id "$vol" \
            > /dev/null

	echo -e "\nEBS volume $vol_id has been deleted."
    fi
}





# Main function ================================================================

main() {
  
    # Check for old instances/volumes
    if "$CHILD_LOCK"
    then
        check_for_undead
    fi



    # Transfer data to S3 bucket
    if "$TRANSFER_DATA"
    then
        transfer_to_s3 \
            "$INPUT" \
            "$S3"
    fi



    # Launch EC2 instance and retrieve ec2_id
    launch_ec2 \
        "$AMI" \
        "$EC2_TYPE" \
        "$ZONE" \
        "$SECURITY_GROUP" \
        "$SUBNET"



    # Check EC2 instance connectivity and retrieve EC2_SSH
    get_dns "$ec2_id"



    # Create EBS volume and retrieve vol_id
    create_ebs \
        "$INPUT" \
        "$ec2_id" \
        "$ZONE" \
        "$S3"



    # Transfer data from S3 bucket to EC2 instance
    project_name="$PROJECT"_$(date +%Y%m%d-%H%M%S)

    transfer_to_ec2 \
        "$S3" \
        "$vol_id" \
        "$project_name"

    if "$MANUAL_MODE"
    then
        exit 0
    fi



    # Run cellranger and transfer results to S3 bucket
    aws="/home/$EC2_USER/.local/bin/aws"

    echo -e "\nBeginning Cell Ranger run..."

    ssh -i "$KEY" "$EC2_SSH" \
        bash "/home/$EC2_USER/PIPELINE/snakecharmer.sh"

    echo -e "\nCell Ranger run has completed, transferring results to S3 bucket $S3."
    
    ssh -i "$KEY" "$EC2_SSH" \
        "$aws" s3 cp /mnt/EBS/RESULTS "$S3" --recursive



    # Terminate EC2 instance and delete access keys
    clean_up \
        "$ec2_id" \
        "$vol_id"
}



main



