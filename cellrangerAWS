#! /bin/bash

set -x -o pipefail



# Inputs
PROJECT_NAME="cellranger"
EC2_TYPE="t2.2xlarge"
AMI="ami-0c426827ef99292f7"
VOLUME_SIZE=300
CREATE_VOL=true
ZONE="us-west-2a"
SECURITY_GROUP="sg-00cf9318d9719735d"
SUBNET="subnet-083a3a855e543ee1c"
USER="ubuntu"



# Usage message
usage() {
    echo """

USAGE
$0 [OPTIONS]

OPTIONS
-h, display this message
-i, input directory containing fastq files and config.yaml or ID for snapshot containing data
-o, output directory
-p, project name
-k, path to AWS private key
-t, EC2 instance type (default is $EC2_TYPE)
-s, size of EBS volume (default is $VOLUME_SIZE GB)
-a, ID for machine image configured to run cellranger (default is $AMI)

    """
}



# Parse arguments
while getopts ":hi:o:p:k:t:s:a:" args
do
    case "$args" in
        h)
            usage
            exit 0
            ;;
        i) 
            INPUT="$OPTARG"

	    if [[ "$INPUT" =~ ^vol\-[[:alnum:]]+$ ]]
            then
                CREATE_VOL=false
            fi
            ;;
        o)
            OUTPUT="$OPTARG"
            ;;
        p)
            PROJECT_NAME="$OPTARG"
            ;;
        k)
            KEY="$OPTARG"
            ;;
        t)
            EC2_TYPE="$OPTARG"
            ;;
        s)
            VOLUME_SIZE="$OPTARG"
            ;;
        a)
            AMI="$OPTARG"
            ;;
        :)
            echo -e "\nERROR: -$OPTARG requires an argument"

            usage
	    exit 1
            ;;
	*) 
            usage
	    exit 1
            ;;
    esac
done

if [[ -z "$INPUT" || -z "$OUTPUT" || -z "$KEY" ]]
then
    echo -e "\nERROR: arguments are required for -i, -o, and -k"

    usage
    exit 1
fi



# Function to check instance/volume state
check_state() {
    local input_fun=$1

    for i in $(seq 1 60)
    do
        state=$("$input_fun")

        if [[ ! -z $state ]]
        then
            break
        fi

        sleep 5
    done
}



# Launch EC2 instance
# When using default VPC the instance does not have internet access. To resolve
# this create new VPC, security group, subnet, and internet gateway. Activate
# public IP for subnet, activate public DNS for VPC, attach VPC to internet
# gateway, modify route table for VPC.
key_name=$(basename -s .pem "$KEY")

ec2_id=$(
    aws ec2 run-instances \
        --image-id "$AMI" \
        --instance-type "$EC2_TYPE" \
	--placement AvailabilityZone="$ZONE" \
	--key-name "$key_name" \
        --associate-public-ip-address \
        --security-group-ids "$SECURITY_GROUP" \
        --subnet-id "$SUBNET" \
        | grep -E -o "InstanceId\": \"i\-[[:alnum:]]+" \
        | grep -E -o "i\-[[:alnum:]]+"
)

running_ec2() {
    aws ec2 describe-instances \
        --instance-id "$ec2_id" \
        --filters Name=instance-state-name,Values=running \
        | grep -o "$ec2_id"
}

check_state running_ec2



# Retrieve public DNS for ssh
# Public DNS is not listed in run-instances output
pub_dns=$(
    aws ec2 describe-instances \
        --instance-id "$ec2_id" \
        | grep -E -o "ec2\-[[:alnum:]\.\-]+compute.amazonaws.com" \
        | head -n 1
)

ssh="$USER@$pub_dns"



# Create and attach EBS volume
if [[ "$CREATE_VOL" == true ]]
then
    vol_id=$(
        aws ec2 create-volume \
            --availability-zone "$ZONE" \
	    --size "$VOLUME_SIZE" \
	    | grep -E -o "vol\-[[:alnum:]]+"
    )

else
    snap_id="$INPUT"

    vol_id=$(
        aws ec2 create-volume \
            --availability-zone "$ZONE" \
            --snapshot-id "$snap_id" \
	    | grep -E -o "vol\-[[:alnum:]]+"
    )
fi

avail_vol() {
    aws ec2 describe-volumes \
        --volume-ids "$vol_id" \
        --filters Name=status,Values=available \
        | grep -o "$vol_id"
}

check_state avail_vol

aws ec2 attach-volume \
    --instance-id "$ec2_id" \
    --volume-id "$vol_id" \
    --device /dev/sdh

attach_vol() {
    aws ec2 describe-volumes \
        --volume-ids "$vol_id" \
        --filters Name=attachment.status,Values=attached \
        | grep -o "$vol_id"
}

check_state attach_vol

sleep 60



# Retrieve device path for EBS volume
dev_path=$(
    ssh -o "StrictHostKeyChecking no" -i "$KEY" "$ssh" \
        sudo fdisk -l \
            | grep -E -o "^Disk .+$VOLUME_SIZE GiB" \
            | grep -E -o "/dev/[[:alnum:]]+" \
            2>&1
)



# Format new EBS volume
if [[ "$CREATE_VOL" == true ]]
then
    ssh -o "StrictHostKeyChecking no" -i "$KEY" "$ssh" bash <<EOF
        sudo parted "$dev_path" mklabel gpt
	sudo parted -a opt "$dev_path" mkpart primary ext4 0% 100%
	sudo mkfs.ext4 -F "$dev_path"
EOF

    #ssh -o "StrictHostKeyChecking no" -i "$KEY" "$ssh" \
    #    sudo parted "$dev_path" mklabel gpt
    #ssh -i "$KEY" "$ssh" \
    #    sudo parted -a opt "$dev_path" mkpart primary ext4 0% 100%
    #ssh -i "$KEY" "$ssh" \
    #    sudo mkfs.ext4 -F "$dev_path"
fi



# Mount EBS volume and transfer data
ssh -o "StrictHostKeyChecking no" -i "$KEY" "$ssh" bash <<EOF
    sudo mkdir -p /mnt/EBS
    sudo mount "$dev_path" /mnt/EBS
    sudo mkdir -p /mnt/EBS/DATA
    sudo chmod -R 777 /mnt
EOF

#ssh -o "StrictHostKeyChecking no" -i "$KEY" "$ssh" \
#    sudo mkdir -p /mnt/EBS
#ssh -i "$KEY" "$ssh" \
#    sudo mount "$dev_path" /mnt/EBS
#ssh -i "$KEY" "$ssh" \
#    sudo mkdir -p /mnt/EBS/DATA
#ssh -i "$KEY" "$ssh" \
#    sudo chmod -R 777 /mnt

if [[ "$CREATE_VOL" == true ]]
then
    scp -i "$KEY" "$INPUT"/*.fastq.gz "$ssh":/mnt/EBS/DATA
    scp -i "$KEY" "$INPUT"/config.yaml "$ssh":/mnt/EBS/DATA
fi



# Run cellranger
ssh -i "$KEY" "$ssh" \
    bash /home/"$USER"/PIPELINE/RUN.sh



# Transfer results to local machine
scp -i "$KEY" "$ssh":/home/"$USER"/PIPELINE/RESULTS/* "$OUTPUT"



# REMEMBER TO REMOVE THIS!!!
exit 0



# Terminate EC2 instance
aws ec2 terminate-instances \
    --instance-ids "$ec2_id"

if [[ "$CREATE_VOL" == true ]]
then
    snap_id=$(
        aws ec2 create-snapshot \
            --description "$PROJECT_NAME" \
            --volume-id "$vol_id" \
	    | grep -E -o "snap\-[[:alnum:]]+"
    )
fi

echo -e "\nData can be accessed with snapshot ID: $snap_id\n"

aws ec2 delete-volume \
    --volume-id "$vol_id"



